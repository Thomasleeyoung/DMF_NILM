{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import re\n",
    "import scipy\n",
    "import os\n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import json\n",
    "import pickle\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "import model_functions as mf\n",
    "import argparse\n",
    "import itertools\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting current time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#defining the parser to read arguments from command line\n",
    "parser = argparse.ArgumentParser(\"Arg parser\")\n",
    "parser.add_argument(\"--idx\") \n",
    "args = vars(parser.parse_args())\n",
    "\n",
    "job_idx = int(args[\"idx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets all possible combinations of changeable parameters\n",
    "###COMMENT THIS NEXT LINE OUT\n",
    "job_idx = 0 ##################################\n",
    "#################################\n",
    "algorithms = [\"benchmark\", \"NN\"]\n",
    "datasets = [\"plaid\", \"whited\", \"both\"]\n",
    "input_sources = [\"AC\", \"RMS\"]\n",
    "combinations = list(itertools.product(algorithms, datasets, input_sources))\n",
    "\n",
    "job_params = combinations[job_idx]\n",
    "algorithm = job_params[0]\n",
    "dataset = job_params[1]\n",
    "input_source = job_params[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#important parameters\n",
    "test_samples = 5000 #number of test examples\n",
    "original_freq = 5000\n",
    "sample_size_range = [50, 100, 500, 1000] #range of training samples per class\n",
    "save_path = \"C:/Users/tslee/IRP Data/DMF_Internship/DATA/\"\n",
    "general_report_path = \"C:/Users/tslee/IRP Data/DMF_Internship/DATA/\"\n",
    "\n",
    "#save_path = \"/user/work/iw20981/DMF_work/DATA/\"\n",
    "#general_report_path = \"/user/work/iw20981/DMF_work/third_run/\"\n",
    "\n",
    "#assigning relevant input type\n",
    "#probably don't need this, just have the input files labelled accordingly\n",
    "if input_source==\"AC\":\n",
    "    freq_range = [150, 500, 1000] #range of freqs in Hz\n",
    "    base_report_path = general_report_path + \"AC_\"\n",
    "elif input_source==\"RMS\":\n",
    "    freq_range = [10, 50, 100] #range of freqs in Hz\n",
    "    base_report_path = general_report_path + \"RMS_\"\n",
    "\n",
    "#reshaping function\n",
    "def arr_reshape(data, labels):\n",
    "    rows = data.shape[0]\n",
    "    for i in data.shape[2:]:\n",
    "        rows *= i\n",
    "    tpose_tuple = list(np.arange(data.ndim))[::-1][:-2]\n",
    "    tpose_tuple.extend([0, 1])\n",
    "    tpose_tuple = tuple(tpose_tuple)\n",
    "    data_reshaped = data.transpose(tpose_tuple).reshape((rows, data.shape[1]))\n",
    "    labels_duplicated = np.reshape(np.tile(labels, int(rows/data.shape[0])), (-1, 1))\n",
    "    return data_reshaped, labels_duplicated\n",
    "#classes\n",
    "whited_classes = np.array(['AC', 'CFL', 'Fan', 'Fridge', 'HairDryer',\n",
    "                            'Heater', 'LightBulb', 'Laptop', 'Microwave', 'VacuumCleaner',\n",
    "                            'WashingMachine'])\n",
    "plaid_classes = np.array(['Air Conditioner', \"Compact Fluorescent Lamp\", 'Fan', 'Fridge', 'Hairdryer',\n",
    "                            'Heater', \"Incandescent Light Bulb\", 'Laptop', 'Microwave', 'Vacuum',\n",
    "                            'Washing Machine'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading in relevant dataset\n",
    "if dataset==\"plaid\":\n",
    "    loaded_data = np.load((save_path + \"PLAID_5000Hz_\"+ input_source +\"_AUGMENTED_data.npy\"), allow_pickle=True)\n",
    "    loaded_labels = np.load((save_path + \"PLAID_5000Hz_labels.npy\"), allow_pickle=True).reshape((-1,))\n",
    "    #flattening the data into 2D, along with duplicating corresponding labels\n",
    "    data_reshaped, labels_duplicated = arr_reshape(loaded_data, loaded_labels)\n",
    "    del(loaded_data)\n",
    "    #assigning report_path\n",
    "    report_path = base_report_path + \"plaid_reports/\"\n",
    "    pass\n",
    "\n",
    "elif dataset==\"whited\":\n",
    "    loaded_data = np.load((save_path + \"WHITED_5000Hz_\"+ input_source +\"_AUGMENTED_data.npy\"), allow_pickle=True)\n",
    "    loaded_labels = np.load((save_path + \"WHITED_5000Hz_\"+ input_source +\"_labels.npy\"), \n",
    "                            allow_pickle=True).reshape((-1,)).astype(\"<U30\")\n",
    "\n",
    "    whited_mask = np.isin(loaded_labels, whited_classes).reshape((-1,))\n",
    "    loaded_labels = loaded_labels[whited_mask]\n",
    "    loaded_data = loaded_data[whited_mask]\n",
    "    \n",
    "    for idx, class_label in enumerate(whited_classes):\n",
    "        loaded_labels[loaded_labels == class_label] = plaid_classes[idx]\n",
    "        whited_rows = loaded_data.shape[0]\n",
    "\n",
    "    data_reshaped, labels_duplicated = arr_reshape(loaded_data, loaded_labels)\n",
    "    del(loaded_data)\n",
    "    #assigning report_path\n",
    "    report_path = base_report_path + \"whited_reports/\"\n",
    "    pass\n",
    "\n",
    "elif dataset==\"both\":\n",
    "    plaid_data = np.load((save_path + \"PLAID_5000Hz_\"+ input_source +\"_AUGMENTED_data.npy\"), allow_pickle=True)\n",
    "    plaid_labels = np.load((save_path + \"PLAID_5000Hz_labels.npy\"), allow_pickle=True).reshape((-1,))\n",
    "    whited_data = np.load((save_path + \"WHITED_5000Hz_\"+ input_source +\"_AUGMENTED_data.npy\"), allow_pickle=True)\n",
    "    whited_labels = np.load((save_path + \"WHITED_5000Hz_\"+ input_source +\"_labels.npy\"), \n",
    "                            allow_pickle=True).reshape((-1,)).astype(\"<U30\")\n",
    "\n",
    "    #only using whited data whose labels match and changing labels to match plaid\n",
    "    whited_mask = np.isin(whited_labels, whited_classes).reshape((-1,))\n",
    "    clean_whited_data = whited_data[whited_mask]\n",
    "    clean_whited_labels = whited_labels[whited_mask]\n",
    "    for idx, class_label in enumerate(whited_classes):\n",
    "        clean_whited_labels[clean_whited_labels == class_label] = plaid_classes[idx]\n",
    "    del(whited_data)\n",
    "\n",
    "    #Unravelling 5D data arrs to 2D, along with duplicating labels accordingly\n",
    "    plaid_reshaped, plaid_labels_duplicated = arr_reshape(plaid_data, plaid_labels)\n",
    "    del(plaid_data)\n",
    "    whited_reshaped, whited_labels_duplicated = arr_reshape(clean_whited_data, clean_whited_labels)\n",
    "    del(clean_whited_data)\n",
    "\n",
    "    #combining PLAID and WHITED data into one array\n",
    "    data_reshaped = np.vstack((plaid_reshaped, whited_reshaped))\n",
    "    del(plaid_reshaped, whited_reshaped)\n",
    "    labels_duplicated = np.vstack((plaid_labels_duplicated, whited_labels_duplicated))\n",
    "    del(plaid_labels_duplicated, whited_labels_duplicated)\n",
    "\n",
    "    #assigning report_path\n",
    "    report_path = base_report_path + \"plaid_and_whited_reports/\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard preprocessing step applied to all data\n",
    "#normalising the data\n",
    "def norm_func(row):\n",
    "    return (row-row.mean())/row.std()\n",
    "norm_data = np.apply_along_axis(norm_func, 1, data_reshaped)\n",
    "del(data_reshaped)\n",
    "\n",
    "#getting random indices for the test set\n",
    "test_indices = np.random.choice(norm_data.shape[0], test_samples, replace=False)\n",
    "#indices not in test set\n",
    "X_remaining_indices = np.delete(np.arange(norm_data.shape[0]), test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoding the labels as required by Xgboost and NNs\n",
    "def label_conversion(labels):\n",
    "    Y_labels = LabelBinarizer()\n",
    "    Y_labels.fit(labels)\n",
    "    label_classes = Y_labels.classes_\n",
    "    Y_labels = Y_labels.transform(labels)\n",
    "    return Y_labels, label_classes\n",
    "#get number of expected labels\n",
    "binarizer = LabelBinarizer()\n",
    "binarizer.fit(labels_duplicated)\n",
    "label_classes = binarizer.classes_\n",
    "Y_onehot = binarizer.transform(labels_duplicated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#carrying out relevant algorithm\n",
    "if algorithm==\"NN\":\n",
    "    #creating an error log file if it doesn't exist already\n",
    "    #if \"NN_log.txt\" not in glob((report_path + \"*\")):\n",
    "    with open((report_path + \"NN_log.txt\"), \"w+\") as err_file:\n",
    "        err_file.write(\"Tests: \\n\")\n",
    "\n",
    "    for freq in freq_range:\n",
    "        for sample_size in sample_size_range:\n",
    "            \"\"\"#get indices for the correct number of training samples from each class\n",
    "            indices_list = []\n",
    "            for label in label_classes:\n",
    "                #indices of available instances from each class\n",
    "                label_indices = np.nonzero(labels_duplicated[X_remaining_indices] == label)[0]\n",
    "                #get certain number of samples from each class\n",
    "                label_indices = list(np.random.choice(label_indices, sample_size))\n",
    "                indices_list.extend(label_indices)\n",
    "            train_indices = np.asarray(indices_list)\"\"\"\n",
    "            \n",
    "            #Method to take n total samples which are randomly selected from total sample set\n",
    "            total_samples = sample_size*len(label_classes)\n",
    "            train_indices = np.random.choice(X_remaining_indices, total_samples)\n",
    "\n",
    "            #gets the indices of the current training set, ensuring not to use the same indices as the test set\n",
    "            downsample_factor = int(original_freq/freq)\n",
    "            rounded_rows = downsample_factor*freq #in case the new frequency is not exactly divisible\n",
    "            X_train = norm_data[train_indices, :rounded_rows]\n",
    "            X_train = np.mean(np.reshape(X_train, (X_train.shape[0], -1, downsample_factor)), axis=2)\n",
    "            \n",
    "            #onehot encode labels\n",
    "            Y_onehot, label_classes = label_conversion(labels_duplicated)\n",
    "            #Y_train = np.reshape(labels_duplicated[train_indices], (-1, ))\n",
    "            Y_train_onehot = Y_onehot[train_indices]\n",
    "\n",
    "            X_test = np.mean(np.reshape(norm_data[test_indices, :rounded_rows], (test_indices.shape[0], -1, downsample_factor)), axis=2)\n",
    "            \n",
    "            Y_test = labels_duplicated[test_indices, 0]\n",
    "            Y_test_onehot = Y_onehot[test_indices, :]\n",
    "            \n",
    "            #print(\"Using \" + str(sample_size) + \" training samples,\" + \" at \" + str(freq) + \" Hz\")\n",
    "            #try to execute NN function, or add to error log file\n",
    "            try: #Neural Networks\n",
    "                test_time = time.time()\n",
    "                mf.NN_func(X_train, X_test, Y_train_onehot, Y_test_onehot, label_classes, \n",
    "                            report_path, num_epochs=1500)\n",
    "                with open((report_path + \"NN_log.txt\"), \"a+\") as log_file:\n",
    "                     log_file.write(\"NN \" + str(sample_size) + \" sample size \" + \"at \" + str(freq) + \" Hz :\" \n",
    "                                    + str(time.time()-test_time) + \" seconds.\" + \"\\n\")\n",
    "                     log_file.write(\"_\" *10+ \"\\n\")\n",
    "                print(\"NN Saved\")\n",
    "            except Exception as error:\n",
    "                with open((report_path + \"NN_log.txt\"), \"a+\") as err_file:\n",
    "                    err_file.write(\"NN \" + str(sample_size) + \" sample size \" + \"at \" + str(freq) + \" Hz \\n\")\n",
    "                    err_file.write(\"AN ERROR OCCURED: \" + str(error) + \"\\n\")\n",
    "                    err_file.write(\"_\" *10+ \"\\n\")\n",
    "            #break\n",
    "        #break\n",
    "    with open((report_path + \"NN_log.txt\"), \"a+\") as err_file:\n",
    "                err_file.write(\"Total Run Time: \" + str(time.time() - start_time) + \" seconds\")\n",
    "\n",
    "elif algorithm==\"benchmark\":\n",
    "    #creating an error log file if it doesn't exist already\n",
    "    #if \"benchmark_log.txt\" not in glob((report_path + \"*\")):\n",
    "    with open((report_path + \"benchmark_log.txt\"), \"w+\") as err_file:\n",
    "        err_file.write(\"Tests: \\n\")\n",
    "\n",
    "    for freq in freq_range:\n",
    "        for sample_size in sample_size_range:\n",
    "            \"\"\"#get indices for the correct number of training samples from each class\n",
    "            indices_list = []\n",
    "            for label in label_classes:\n",
    "                #indices of available instances from each class\n",
    "                label_indices = np.nonzero(labels_duplicated[X_remaining_indices] == label)[0]\n",
    "                #get certain number of samples from each class\n",
    "                label_indices = list(np.random.choice(label_indices, sample_size))\n",
    "                indices_list.extend(label_indices)\n",
    "            train_indices = np.asarray(indices_list)\"\"\"\n",
    "\n",
    "            #Method to take n total samples which are randomly selected from total sample set\n",
    "            total_samples = sample_size*len(label_classes)\n",
    "            train_indices = np.random.choice(X_remaining_indices, total_samples)\n",
    "            \n",
    "            #gets the indices of the current training set, ensuring not to use the same indices as the test set\n",
    "            downsample_factor = int(original_freq/freq)\n",
    "            rounded_rows = freq*downsample_factor\n",
    "            X_train = norm_data[train_indices, :]\n",
    "            X_train = np.mean(np.reshape(X_train[:, :rounded_rows], (X_train.shape[0], -1, downsample_factor)), axis=2)\n",
    "            \n",
    "            Y_train = np.reshape(labels_duplicated[train_indices], (-1, ))\n",
    "            Y_train_onehot = Y_onehot[train_indices, :]\n",
    "\n",
    "            X_test = np.mean(np.reshape(norm_data[test_indices, :rounded_rows], (test_indices.shape[0], -1, downsample_factor)), axis=2)\n",
    "            \n",
    "            #onehot encode labels\n",
    "            Y_onehot, label_classes = label_conversion(labels_duplicated)\n",
    "            Y_test = labels_duplicated[test_indices, 0]\n",
    "            Y_test_onehot = Y_onehot[test_indices, :]\n",
    "            \n",
    "            #print(\"Using \" + str(sample_size) + \" training samples,\" + \" at \" + str(freq) + \" Hz\")\n",
    "            #try to execute each function, or add to error log file\n",
    "            try: #KNN\n",
    "                test_time = time.time()\n",
    "                mf.knn_func(X_train, X_test, Y_train, Y_test, report_path)\n",
    "                with open((report_path + \"benchmark_log.txt\"), \"a+\") as log_file:\n",
    "                     log_file.write(\"KNN \" + str(sample_size) + \" sample size \" + \"at \" + str(freq) + \" Hz :\" \n",
    "                                    + str(time.time()-test_time) + \" seconds.\" + \"\\n\")\n",
    "                     log_file.write(\"_\" *10+ \"\\n\")\n",
    "                print(\"KNN Saved\")\n",
    "            except Exception as error:\n",
    "                with open((report_path + \"benchmark_log.txt\"), \"a+\") as err_file:\n",
    "                    err_file.write(\"KNN \" + str(sample_size) + \" sample size \" + \"at \" + str(freq) + \" Hz \\n\")\n",
    "                    err_file.write(\"AN ERROR OCCURED: \" + str(error) + \"\\n\")\n",
    "                    err_file.write(\"_\" *10+ \"\\n\")\n",
    "\n",
    "            try: #SVM\n",
    "                test_time = time.time()\n",
    "                mf.svm_func(X_train, X_test, Y_train, Y_train_onehot, Y_test, report_path)\n",
    "                with open((report_path + \"benchmark_log.txt\"), \"a+\") as log_file:\n",
    "                     log_file.write(\"SVM \" + str(sample_size) + \" sample size \" + \"at \" + str(freq) + \" Hz :\" \n",
    "                                    + str(time.time()-test_time) + \" seconds.\" + \"\\n\")\n",
    "                     log_file.write(\"_\" *10+ \"\\n\")\n",
    "                print(\"SVM Saved\")\n",
    "            except Exception as error:\n",
    "                with open((report_path + \"benchmark_log.txt\"), \"a+\") as err_file:\n",
    "                    err_file.write(\"SVM \" + str(sample_size) + \" sample size \" + \"at \" + str(freq) + \" Hz \\n\")\n",
    "                    err_file.write(\"AN ERROR OCCURED: \" + str(error) + \"\\n\")\n",
    "                    err_file.write(\"_\" *10 + \"\\n\")\n",
    "\n",
    "            try: #XGBOOST\n",
    "                test_time = time.time()\n",
    "                mf.xgb_func(X_train, X_test, Y_train_onehot, Y_test_onehot, label_classes, report_path)\n",
    "                with open((report_path + \"benchmark_log.txt\"), \"a+\") as log_file:\n",
    "                     log_file.write(\"XGBoost \" + str(sample_size) + \" sample size \" + \"at \" + str(freq) + \" Hz :\" \n",
    "                                    + str(time.time()-test_time) + \" seconds.\" + \"\\n\")\n",
    "                     log_file.write(\"_\" *10+ \"\\n\")\n",
    "                print(\"XGBoost Saved\")\n",
    "            except Exception as error:\n",
    "                with open((report_path + \"benchmark_log.txt\"), \"a+\") as err_file:\n",
    "                    err_file.write(\"XGBoost \" + str(sample_size) + \" sample size \" + \"at \" + str(freq) + \" Hz \\n\")\n",
    "                    err_file.write(\"AN ERROR OCCURED: \" + str(error) + \"\\n\")\n",
    "                    err_file.write(\"_\" *10+ \"\\n\")\n",
    "\n",
    "            #break\n",
    "        #break\n",
    "    with open((report_path + \"benchmark_log.txt\"), \"a+\") as err_file:\n",
    "                err_file.write(\"Total Run Time: \" + str(time.time() - start_time) + \" seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
